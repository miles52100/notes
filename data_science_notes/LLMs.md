# Large Language Models, LLMs

The term *Foundational Model* coined by Stanford researchers to introduce a new cateogry of ML.

They defined FM as

    models trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks.

See [here](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

## Hugging face leader board

Currently topping it is Falcon 180B LLM, developed by Technology Innovation Institute, described as a *leading global research center dedicated to pushing the frontiers of knowledge.*
It is centered in Abu Dhabi, UAE.

## Tokenization

[Tokenization algorithms explained](https://towardsdatascience.com/tokenization-algorithms-explained-e25d5f4322ac)

## RAG

**Retrieval-Augmented Generation**

The process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response.

### How does it work?

With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.

See [References/RAG](#references) for more details

### LLaMa

Meta's LLM numerous descendants, including Vicuna.
Was the most performant but succeeded by LLaMa2

---
---

### References

[puresinsight.com](file:///Users/miles52100/Downloads/E-Book-Unleashing-AI-Powered-Search-Pureinsights.pdf)

[Wikipedia list of LLMs](https://en.wikipedia.org/wiki/Large_language_model)

[Impact of Tokenization on LM](https://arxiv.org/pdf/2204.08832.pdf)

[RAG](https://aws.amazon.com/what-is/retrieval-augmented-generation/)